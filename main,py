from base_class import BaseScraper, JobResult
from urllib.parse import quote_plus
from selenium.webdriver.common.by import By
import logging

class RadiantScraper(BaseScraper):
    def __init__(self, radius=10, location='El Segundo, CA', start=0, logger=None):
        base_url = 'https://boards.greenhouse.io/radiant'
        super().__init__(base_url, radius, location, start, logger)

    def assemble_url(self):
        self.url = 'https://boards.greenhouse.io/radiant'
        self.logger.info(f"Assembled URL: {self.url}")

    def next_page(self, job_results):
        return super().next_page(job_results)

    def get_jobs(self) -> list:
        job_results = []
        try:
            links = self.browser.find_elements(By.TAG_NAME, 'a')
            job_links = [link.get_attribute('href') for link in links if 'jobs' in link.get_attribute('href')]
        except Exception as e:
            self.logger.error(f"Error: {e}")
            return job_results

        for href in job_links:
            self.random_delay(0.1, 0.5)
            try:
                self.browser.get(href)

                title_element = self.browser.find_element(By.TAG_NAME, 'h1')
                title = title_element.text if title_element else "N/A"

                # Extracting the text content of the sections
                responsibilities = self.extract_children_span_text(self.browser.find_element(By.XPATH, '/html/body/div[1]/div/div/div[3]/ul[1]'))
                qualifications = self.extract_children_span_text(self.browser.find_element(By.XPATH, '/html/body/div[1]/div/div/div[3]/ul[2]'))
                desired_qualifications = self.extract_children_span_text(self.browser.find_element(By.XPATH, '/html/body/div[1]/div/div/div[3]/ul[3]'))

                # Combine qualifications and desired qualifications
                all_qualifications = qualifications + desired_qualifications

                job_result = JobResult(
                    company='Radiant',
                    title=title,
                    industry="N/A",
                    responsibilities=responsibilities,
                    qualifications=all_qualifications,
                    other="N/A"
                )
                job_results.append(job_result)

                # Navigate back to the main jobs page
                self.browser.back()

            except Exception as e:
                self.logger.error(f"Error: {e}")

        self.logger.info(f"Found {len(job_results)} jobs")
        return job_results
    
    def extract_children_span_text(self, element):
        text = []
        try:
            children = element.find_elements(By.TAG_NAME, 'span')
            for child in children:
                text.append(child.text)
            return "\n".join(text)
        except Exception as e:
            self.logger.error(f"Span parsing error: {e}")
            return "N/A"

if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    scraper = RadiantScraper(logger=logger)
    job_results = scraper.get_jobs()
    scraper.close_browser()

    for job in job_results:
        print(job)
